---
layout: post
title: "ğŸ™ Ceph Storage Cluster Nedir? DetaylÄ± inceliyorum"
date: 2026-02-04 04:25
by: faruk-guler
comments: true
categories: [Storage]
---
![Ceph Logosu](https://farukguler.com/assets/post_images/ceph-storage-clusters-ystems.webp)
## Ceph Storage Cluster: UÃ§tan Uca Kurulum Rehberi

Bu dokÃ¼manÄ±, Ceph Reef (veya Quincy) sÃ¼rÃ¼mÃ¼nÃ¼ baz alarak, modern standart olan **cephadm (Containerized Deployment)** yÃ¶ntemiyle hazÄ±rladÄ±m. Senin Docker ve Linux aÄŸ bilgine gÃ¼venerek, sadece "kur geÃ§" deÄŸil, "neden bÃ¶yle yapÄ±yoruz" detaylarÄ±na da gireceÄŸim.

> **Not:** Bu kurulum Production-Ready (CanlÄ± Ortam) standartlarÄ±na en yakÄ±n ev laboratuvarÄ±/kÃ¼Ã§Ã¼k iÅŸletme senaryosudur.

## ğŸ—ï¸ 1. Mimari ve Gereksinimler (Planlama)

Ceph kurmadan Ã¶nce masaya koyman gereken mimari ÅŸu olmalÄ±. En az **3 Node (Sunucu)** kullanacaÄŸÄ±z.

### DonanÄ±m Ã–nerisi (Node BaÅŸÄ±na)

* **CPU:** En az 4 Ã‡ekirdek (Replikasyon CPU yer).
* **RAM:** En kritik nokta. OSD (Disk) baÅŸÄ±na yaklaÅŸÄ±k 2-4 GB RAM hesapla. 3 Diskli bir sunucu iÃ§in 16GB RAM idealdir.
  * **Production FormÃ¼lÃ¼:** `Total RAM = [OSD sayÄ±sÄ±] x (4 GB + 8 GB) + 16 GB`
  * Ã–rnek: 3 OSD â†’ 3 x 12 GB + 16 GB = **52 GB** Ã¶nerilir.
* **Diskler:**
  * 1x SSD/NVMe -> Ä°ÅŸletim Sistemi (OS) iÃ§in.
  * 2x veya daha fazla HDD/SSD -> Ceph Verisi (OSD) iÃ§in. (**Ã–NEMLÄ°:** Bu diskler ham/RAW olacak, formatlanmayacak, partition olmayacak!)

### Network (Ä°ncelik 1)

Ceph'te iki trafik vardÄ±r:

1. **Public Network:** Ä°stemcilerin (VM'ler, PC'ler) baÄŸlandÄ±ÄŸÄ± aÄŸ.
2. **Cluster Network:** OSD'lerin arka planda veri kopyaladÄ±ÄŸÄ± (Replication/Recovery) aÄŸ.

> **Pro Tip:** MÃ¼mkÃ¼nse bunlarÄ± fiziksel olarak ayÄ±rÄ±n veya VLAN ile ayÄ±rÄ±n. Replikasyon trafiÄŸi, public hattÄ± tÄ±kamamalÄ±.

### Ã–rnek Lab OrtamÄ±

* **Node 1 (Admin/Monitor):** 192.168.1.10
* **Node 2 (OSD Node):** 192.168.1.11
* **Node 3 (OSD Node):** 192.168.1.12
* **OS:** Ubuntu 22.04 LTS veya Debian 12 (Rocky Linux da olur ama Debian tabanÄ±na aÅŸinasÄ±n).

---

## ğŸ› ï¸ 2. Ã–n HazÄ±rlÄ±k (TÃ¼m Node'larda YapÄ±lacak)

Bu komutlarÄ± **TÃœM** sunucularda Ã§alÄ±ÅŸtÄ±r.

### A. Hostname ve DNS AyarÄ±

Ceph, node'larÄ± isimle tanÄ±r. `/etc/hosts` dosyasÄ±na hepsini ekle:

```bash
# /etc/hosts dosyasÄ±na ekle:
192.168.1.10 node1.local node1
192.168.1.11 node2.local node2
192.168.1.12 node3.local node3
```

### B. Zaman Senkronizasyonu (Kritik Ä°ncelik) âš ï¸

Ceph'in toleransÄ± milisaniyelerdir. Saatler kayarsa cluster Ã§Ã¶ker (Clock Skew hatasÄ±).

```bash
sudo apt update && sudo apt install -y chrony lvm2 curl podman
# Podman veya Docker ÅŸarttÄ±r. Ceph artÄ±k konteyner iÃ§inde Ã§alÄ±ÅŸÄ±r.
```

### C. Swap'Ä± Kapatmak (Performans Ä°nceliÄŸi)

Ceph RAM'i sever. RAM bitince diske (Swap) yazarsa performans yerle bir olur.

```bash
sudo swapoff -a
# /etc/fstab iÃ§inden swap satÄ±rÄ±nÄ± kalÄ±cÄ± olarak sil veya yorum satÄ±rÄ± yap (#).
```

---

## ğŸš€ 3. Cluster'Ä± BaÅŸlatma (Sadece Ä°lk Node - Node1)

ArtÄ±k sadece Node 1 Ã¼zerindeyiz. BurasÄ± bizim yÃ¶netim merkezimiz olacak.

### A. Cephadm ve CLI AraÃ§larÄ±nÄ± YÃ¼kle

```bash
# DoÄŸru linkten cephadm scriptini indir (Resmi sunucu)
curl --silent --remote-name --location https://download.ceph.com/rpm-reef/el9/noarch/cephadm

# Ã‡alÄ±ÅŸtÄ±rma izni ver
chmod +x cephadm

# Reef sÃ¼rÃ¼mÃ¼ iÃ§in repolarÄ± sisteme ekle
sudo ./cephadm add-repo --release reef

# ceph-common paketini yÃ¼kle
sudo ./cephadm install ceph-common
```

### B. Bootstrap (BÃ¼yÃ¼k Patlama) ğŸ’¥

Bu komut; ilk Monitor (MON), ilk Manager (MGR) daemonlarÄ±nÄ± ayaÄŸa kaldÄ±rÄ±r ve bir Dashboard oluÅŸturur.

```bash
# Public network IP adresini belirtiyoruz.
# Opsiyonel: EÄŸer cluster trafiÄŸini ayÄ±rmak istersen --cluster-network 10.10.10.0/24 ekle.
sudo cephadm bootstrap --mon-ip 192.168.1.10
```

> Bu iÅŸlem bittiÄŸinde sana bir **Dashboard URL, KullanÄ±cÄ± AdÄ± (admin) ve Åifre** verecek. Bunu bir yere not et!
> **Pro Tip - Komut KÄ±sayolu:** Her seferinde `cephadm shell -- ceph` yazmak yerine alias oluÅŸtur:
>
> ```bash
> alias ceph='cephadm shell -- ceph'
> # KalÄ±cÄ± yapmak iÃ§in ~/.bashrc dosyasÄ±na ekle
> echo "alias ceph='cephadm shell -- ceph'" >> ~/.bashrc
> ```

---

## ğŸ”— 4. DiÄŸer SunucularÄ± Cluster'a Ekleme

Cephadm, diÄŸer sunuculara SSH ile baÄŸlanÄ±p onlarÄ± yÃ¶netir.

### A. SSH AnahtarÄ±nÄ± DaÄŸÄ±t

Bootstrap iÅŸlemi sÄ±rasÄ±nda Node1'de Ã¶zel bir SSH key oluÅŸtu (`/etc/ceph/ceph.pub`). Bunu diÄŸer sunuculara kopyalamalÄ±yÄ±z.

```bash
ssh-copy-id -f -i /etc/ceph/ceph.pub root@node2
ssh-copy-id -f -i /etc/ceph/ceph.pub root@node3
```

### B. Node'larÄ± Ekleyin (Orchestrator)

ArtÄ±k Ceph CLI Ã¼zerinden diÄŸer sunucularÄ± havuza dahil ediyoruz.

```bash
# Ã–nce Ceph shell'e gir (Container iÃ§ine girmek gibidir)
sudo cephadm shell

# Node'larÄ± ekle
ceph orch host add node2 192.168.1.11  # Buraya Management/Public IP adresini yaz.
ceph orch host add node3 192.168.1.12

# Durumu kontrol et
ceph orch host ls
```

> Burada 3 sunucuyu da "Status" sÃ¼tununda aktif gÃ¶rmelisin.

---

## ğŸ’¾ 5. OSD (Disk) Ekleme ve Depolama AlanÄ±

Åimdi sunucularÄ±n Ã¼zerindeki boÅŸ diskleri Ceph'e tanÄ±tacaÄŸÄ±z.

### A. Diskleri Kontrol Et

Sistemdeki boÅŸ diskleri Ceph gÃ¶rÃ¼yor mu?

```bash
ceph orch device ls
```

> Ã‡Ä±ktÄ±da `/dev/sdb` gibi diskleri ve "Available: Yes" ibaresini gÃ¶rmelisin. EÄŸer "Available: No" diyorsa diskte partition veya LVM kalÄ±ntÄ±sÄ± vardÄ±r. `wipefs -a /dev/sdb` ile veya inatÃ§Ä± durumlarda `ceph-volume lvm zap /dev/sdb --destroy` ile silmen gerekir.

### B. TÃ¼m BoÅŸ Diskleri OSD Yap (Kolay Yol)

AÅŸaÄŸÄ±daki komut, tÃ¼m node'lardaki tÃ¼m boÅŸ ve uygun diskleri otomatik olarak OSD yapar:

```bash
ceph orch apply osd --all-available-devices
```

### C. GeliÅŸmiÅŸ Disk YapÄ±landÄ±rmasÄ± (Ä°ncelik) ğŸ§ 

EÄŸer "Benim SSD'lerim var, bunlarÄ± HDD'lerin cache'i (WAL/DB) olarak kullanmak istiyorum" dersen (Hybrid OSD), basit komut yerine Drive Group (YAML) kullanmalÄ±sÄ±n.

Ã–rnek (Sadece bilgi amaÃ§lÄ±, yukarÄ±daki komut genelde yeterlidir):

```yaml
service_type: osd
service_id: osd_spec_default
placement:
  host_pattern: '*'
data_devices:
  paths:
    - /dev/sdb
    - /dev/sdc
db_devices:
  paths:
    - /dev/nvme0n1  # HDD'lerin metadata'sÄ± bu hÄ±zlÄ± diske yazÄ±lÄ±r.
```

---

## ğŸ§  6. Servisleri DaÄŸÄ±tma (MDS, RGW)

Ceph sadece blok (RBD) depolama deÄŸildir.

### A. CephFS (Dosya Sistemi - NAS gibi kullanÄ±m) iÃ§in

Metadata Server (MDS) gereklidir.

```bash
# Ã–nce MDS servislerini daÄŸÄ±t
ceph orch apply mds myfs --placement="3 node1 node2 node3"

# ArdÄ±ndan CephFS volume'Ã¼ oluÅŸtur
ceph fs volume create myfs
```

### B. Object Storage (S3 uyumlu) iÃ§in

Rados Gateway (RGW) gereklidir.

```bash
ceph orch apply rgw myrgw --placement="3 node1 node2 node3"
```

---

## ğŸ•µï¸ 7. Ceph'in "Ä°ncelikleri" ve Pro Ä°puÃ§larÄ±

Bir sistemci olarak bilmen gereken ve kurulum rehberlerinde yazmayan sÄ±rlar:

### 1. PG Calc (Placement Groups)

Ceph, veriyi disklere direkt yazmaz. `Veri -> Obje -> PG (Placement Group) -> OSD` zincirini izler.

* **Sorun:** PG sayÄ±sÄ±nÄ± az verirsen veri daÄŸÄ±lmaz, performans dÃ¼ÅŸer. Ã‡ok verirsen CPU patlar.
* **Kural:** OSD baÅŸÄ±na yaklaÅŸÄ±k 100 PG hedeflenir.
* Ceph Reef sÃ¼rÃ¼mÃ¼nde `pg_autoscaler on` olarak gelir, yani Ceph bunu senin yerine hesaplar. Bunu sakÄ±n kapatma!

### 2. Failure Domain (Hata AlanÄ±)

VarsayÄ±lan olarak Ceph "Host" bazlÄ± koruma yapar. Yani 3 sunucun varsa, bir dosyanÄ±n 1. kopyasÄ± Node1'de, 2. kopyasÄ± Node2'de, 3. kopyasÄ± Node3'tedir.

* **Ä°ncelik:** EÄŸer tek sunucuya Ceph kurarsan (Ã¶nermemiÅŸtim), Ceph verileri replike edemez ve `HEALTH_WARN` durumunda kalÄ±r. Bunu dÃ¼zeltmek iÃ§in CRUSH haritasÄ±nÄ± `osd` seviyesine indirmen gerekir (Production'da yapÄ±lmaz).

### 3. Client Keyring

Ä°stemcilerin (Ã–rn: Proxmox veya baÅŸka bir Linux sunucu) Ceph'e baÄŸlanmasÄ± iÃ§in admin yetkisine ihtiyacÄ± yoktur. Her istemci iÃ§in ayrÄ± "Keyring" oluÅŸtur:

```bash
ceph auth get-or-create client.proxmox mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=vms'
```

### 4. Quorum (Split-Brain)

3 Monitor (MON) sunucusu ile baÅŸladÄ±k. EÄŸer 2 sunucu kapanÄ±rsa, kalan 1 sunucu "Ben Ã§oÄŸunluk muyum?" diye bakar. OylarÄ±n %50'sinden fazlasÄ±nÄ± alamazsa Cluster kendini kilitler (Read-Only bile olmaz, durur). Buna **Quorum** denir. Veri bozulmasÄ±nÄ± Ã¶nlemek iÃ§in bu davranÄ±ÅŸ hayati Ã¶nem taÅŸÄ±r.

---

## ğŸ 8. Test Etme

Kurulum bitti, her ÅŸey yeÅŸil. Test edelim.

### Bir Havuz (Pool) OluÅŸtur

```bash
ceph osd pool create testpool 32 32
ceph osd pool application enable testpool rbd
```

### Bir Blok Disk (Image) OluÅŸtur

```bash
rbd create disk1 --size 10G --pool testpool
```

### Diski Map Et (Linux'a baÄŸla)

```bash
rbd map disk1 --pool testpool --name client.admin
# Ã‡Ä±ktÄ±: /dev/rbd0
```

### Formatla ve Kullan

```bash
mkfs.ext4 /dev/rbd0
mount /dev/rbd0 /mnt
```

**Tebrikler!** ArtÄ±k kendi kendine yeten, kendini iyileÅŸtirebilen (Self-healing), Enterprise seviyesinde bir depolama kÃ¼men var. Dashboard'a (`https://192.168.1.10:8443`) girip o meÅŸhur Ceph arayÃ¼zÃ¼ne eriÅŸebilirsiniz.

ğŸ“ **Tam  kapsamlÄ± sÃ¼rÃ¼me eriÅŸmek iÃ§in:**  
ğŸ‘‰ [https://github.com/faruk-guler/Sahara/tree/main/CEPH](https://github.com/faruk-guler/Sahara/tree/main/CEPH)
